{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style Transfer",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzmeCpsgXeQwP7MSVUd8Dc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/mvenouziou/35633b556aca969f7241ae83836c5b4f/style-transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dklI1d0SwqMd"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt \n",
        "import IPython.display as display\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras import layers\n",
        "\n",
        "# import PIL\n",
        "# import pprint\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWg0lHHZMKQ5"
      },
      "source": [
        "# Select Images & Model Parameters\n",
        "\n",
        "SIZE = (224, 224)\n",
        "CONTENT_WEIGHT = 10000\n",
        "STYLE_WEIGHT = .01\n",
        "NUM_ITER = 50\n",
        "LAYERS_WEIGHTS = {1: 0.2,\n",
        "                  4: 0.2,\n",
        "                  7: 0.2,\n",
        "                  12: 0.2,\n",
        "                  17: 0.2,\n",
        "                  18: 0.2,\n",
        "                  }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOufVsiKwzQf"
      },
      "source": [
        "# load pre-trained model\n",
        "# model: ImageNet VGG Very Deep 19\n",
        "\n",
        "def load_base_model(model='VGG19'):\n",
        "    # load pre-trained model\n",
        "    # model: VGG19 or EfficientNetB7\n",
        "\n",
        "    if model == 'VGG19':\n",
        "     source_model = \\\n",
        "           tf.keras.applications.VGG19(include_top=False, weights='imagenet', \n",
        "                input_tensor=None, input_shape=None, pooling=None, \n",
        "                classes=1000, classifier_activation='softmax')\n",
        "        \n",
        "    else:  # model == 'EfficientNetB7'  ##### not yet implemented. \n",
        "        pass\n",
        "    \"\"\"\n",
        "    ##### raises 'ValueError: A merge layer should be called '\n",
        "    \n",
        "      source_model = \\\n",
        "        tf.keras.applications.EfficientNetB7(include_top=False, \n",
        "                weights='imagenet', input_tensor=None, input_shape=None,\n",
        "                pooling=None, classes=1000, classifier_activation='softmax')\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    source_model.trainable = False\n",
        "    return source_model\n",
        "\n",
        "source_model = load_base_model()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd1d8Y6LW4Ry",
        "outputId": "0b5881fb-68ec-434b-8543-528194991755",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "source_model.layers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f15c6241f60>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c61f60f0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c61f6390>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f15c61f67b8>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c61fb2e8>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c61fbe10>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f15c61764a8>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c6176c88>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c617fb00>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c617fbe0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c61887f0>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f15c61916d8>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c6191eb8>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c6197d30>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c61979e8>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c61a1a20>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f15c612c908>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c6134128>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c6134f60>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c613a400>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f15c613ac50>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f15c6143828>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VpksbM6PyfI"
      },
      "source": [
        "def view_generated_image(im_array, view=True):\n",
        "    im_array = im_array.numpy()\n",
        "    image = \\\n",
        "        tf.keras.preprocessing.image.array_to_img(im_array, data_format=None,\n",
        "                                                  scale=True, dtype=None)\n",
        "    if view is True:\n",
        "        plt.imshow(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def prepare_images(image_content, image_style, size=SIZE):\n",
        "    \"\"\"\n",
        "    # format images as tensors usable by model\n",
        "    \"\"\"\n",
        "\n",
        "    # load images\n",
        "    content = plt.imread(image_content)\n",
        "    style = plt.imread(image_style)\n",
        "\n",
        "    # Resize images\n",
        "    content = tf.keras.preprocessing.image.smart_resize(content, size)\n",
        "    style = tf.keras.preprocessing.image.smart_resize(style, size)\n",
        "\n",
        "    # convert to arrays and standardize\n",
        "    content = tf.keras.preprocessing.image.img_to_array(content)/255\n",
        "    style = tf.keras.preprocessing.image.img_to_array(style)/255\n",
        "\n",
        "    # convert to proper shape: (1, height, width, channels)\n",
        "    content = tf.expand_dims(content, axis=0)\n",
        "    style = tf.expand_dims(style, axis=0)\n",
        "\n",
        "    return content, style\n",
        "\n",
        "\n",
        "def initialize_generated_image(input_image, noise_rate):\n",
        "    noise = tf.random.uniform(shape=input_image.shape, minval=-20, maxval=20)\n",
        "    image = noise * noise_rate + input_image * (1 - noise_rate)\n",
        "    return image"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3GSlliTw0MI"
      },
      "source": [
        "### Cost Functions\n",
        "# Difference between input image and generated\n",
        "# Difference between style image and generated"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gicnBEqLw1IX"
      },
      "source": [
        "### Cost Functions\n",
        "# Difference between input image and generated\n",
        "# Difference between style image and generated\n",
        "\n",
        "def style_cost(style_activations, generated_activations, weights):\n",
        "\n",
        "    # initialize cost\n",
        "    cost = 0\n",
        "\n",
        "    for layer in style_activations.keys():\n",
        "        # select content data\n",
        "        style = style_activations[layer]\n",
        "        generated = generated_activations[layer]\n",
        "        weight = weights[layer]\n",
        "\n",
        "        # compute scale factor\n",
        "        height = style.shape[-3]\n",
        "        width = style.shape[-2]\n",
        "        num_channels = style.shape[-1]\n",
        "        scale_factor = 1 / (4 * (height**2) * (width**2) * (num_channels**2))\n",
        "\n",
        "       # compute cost\n",
        "        gram_style = gram_matrix(style)\n",
        "        gram_generated = gram_matrix(generated)\n",
        "\n",
        "        cost += weight * scale_factor * tf.norm(gram_style - gram_generated, ord=2)**2\n",
        "\n",
        "    return cost\n",
        "\n",
        "\n",
        "def gram_matrix(image):\n",
        "    \"\"\"\n",
        "    Computes Gram Matrix\n",
        "    Parameter:\n",
        "    image: tensors of shape (px_width, px_height, channels)\n",
        "    \"\"\"\n",
        "    num_channels = image.shape[-1]\n",
        "    image = tf.reshape(image, [-1, num_channels])\n",
        "    gram = tf.linalg.matmul(image, image, transpose_a=True, transpose_b=False)\n",
        "    return gram\n",
        "\n",
        "\n",
        "def content_cost(image_activations, generated_activations, content_layer):\n",
        "\n",
        "    # select content data\n",
        "    image = image_activations[content_layer]\n",
        "    generated = generated_activations[content_layer]\n",
        "\n",
        "    # compute scale factor\n",
        "    height = image.shape[-3]\n",
        "    width = image.shape[-2]\n",
        "    num_channels = image.shape[-1]\n",
        "    scale_factor = 1 / (4 * height * width * num_channels)\n",
        "\n",
        "    # compute cost\n",
        "    cost = scale_factor * tf.norm(tensor= image - generated, ord='euclidean')\n",
        "\n",
        "    return cost"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyfYRX7xPjto"
      },
      "source": [
        "\n",
        "def learn_activations(input_tensor, orig_model, layers_list):\n",
        "    \"\"\"\n",
        "    Conducts forward pass of given model and stores activations\n",
        "    indicated by layers list.\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize activations container\n",
        "    activations_computed = dict()\n",
        "    num_layers = len(orig_model.layers)  # number of activations to record\n",
        "    layers_list = [1] + layers_list  # adjustment for better looping\n",
        "\n",
        "    # forward pass\n",
        "\n",
        "    x = orig_model.layers[1](input_tensor)\n",
        "\n",
        "    for k in range(1, len(layers_list)):\n",
        "        for i in range(layers_list[k - 1] + 1, layers_list[k] + 1):\n",
        "            x = orig_model.layers[i](x)\n",
        "        # store activation\n",
        "        activations_computed[layers_list[k]] = x\n",
        "\n",
        "    #for i in range(layers_list[-1] + 1, num_layers):  # final step\n",
        "    #    x = orig_model.layers[i](x)\n",
        "    #activations_computed['output'] = x\n",
        "\n",
        "    return activations_computed\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XYa7ZYFayTF"
      },
      "source": [
        "def train_generated_image(init_generated_image, image_activations, \n",
        "                          style_activations, cnn_model, layers_dict, \n",
        "                          content_weight, style_weight, num_iterations):\n",
        "\n",
        "    computed_images_dict = dict()\n",
        "\n",
        "    # get parameters\n",
        "    layers = list(layers_dict.keys())  # lists of CNN layer numbers\n",
        "    content_layer = list(layers_dict.keys())[-1]\n",
        "\n",
        "    # initialize variables\n",
        "    learned_im = tf.Variable(init_generated_image)\n",
        "\n",
        "    # set optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.9, \n",
        "                                         beta_2=0.999, epsilon=1e-1, \n",
        "                                         amsgrad=False, name='Adam')\n",
        "\n",
        "    for iter in range(num_iterations):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(learned_im)\n",
        "\n",
        "            def loss_function():\n",
        "                generated_activations = \\\n",
        "                    learn_activations(learned_im, cnn_model, layers)\n",
        "                # compute loss\n",
        "                loss = content_weight * content_cost(image_activations, \n",
        "                                                     generated_activations, \n",
        "                                                     content_layer) + \\\n",
        "                        style_weight * style_cost(style_activations, \n",
        "                                                  generated_activations, \n",
        "                                                  layers_dict)\n",
        "                return loss\n",
        "\n",
        "        # apply gradients\n",
        "        optimizer.minimize(loss=loss_function, var_list=[learned_im])\n",
        "        learned_im = \\\n",
        "            tf.clip_by_value(learned_im, clip_value_min=0.0, clip_value_max=1.0)\n",
        "        learned_im = tf.Variable(init_generated_image)\n",
        "\n",
        "        # give status updates\n",
        "        if iter % 100 == 0:\n",
        "            print(\"iteration:\", iter)\n",
        "            print(\"loss:\", loss_function().numpy())\n",
        "            # save image to dictionary\n",
        "            computed_images_dict[iter] = learned_im\n",
        "            \n",
        "            if iter % 150 == 0:\n",
        "                learned_image = tf.convert_to_tensor(learned_im * 255)\n",
        "                learned_image = tf.squeeze(learned_image)\n",
        "                display.clear_output(wait=True)\n",
        "                display.display(view_generated_image(learned_image, False))\n",
        "                \n",
        "    print(\"Completed.\")\n",
        "    print(\"loss:\", loss_function().numpy())\n",
        "\n",
        "    learned_image = tf.convert_to_tensor(learned_im * 255)\n",
        "    learned_image = tf.squeeze(learned_image)\n",
        "\n",
        "    return learned_image, computed_images_dict\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCYa1QiHGGtX"
      },
      "source": [
        "def style_transfer(image_content, image_style, layers_dict=LAYERS_WEIGHTS, *kargs):\n",
        "\n",
        "    # prepare images\n",
        "    image_tensor, style_tensor = prepare_images(image_content, image_style)\n",
        "\n",
        "    init_generated_image = initialize_generated_image(image_tensor, noise_rate=.01)\n",
        "    init_generated_image = tf.convert_to_tensor(init_generated_image)\n",
        "\n",
        "    # prepare model parameters\n",
        "    image_model = load_base_model()  # pre-trained image processing CNN\n",
        "    layers = list(layers_dict.keys())  # lists CNN layer numbers for transfer\n",
        "    image_activations = learn_activations(image_tensor, image_model, layers)\n",
        "    style_activations = learn_activations(style_tensor, image_model, layers)\n",
        "\n",
        "    # style transfer model\n",
        "    G_new, computed_images_dict = \\\n",
        "        train_generated_image(init_generated_image, image_activations, \n",
        "                              style_activations, image_model, layers_dict, \n",
        "                              content_weight=CONTENT_WEIGHT, \n",
        "                              style_weight=STYLE_WEIGHT, num_iterations=NUM_ITER)\n",
        "\n",
        "    return G_new, computed_images_dict"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQl2c6poVdTQ",
        "outputId": "0f177ec0-98bf-4b4d-aae6-4c2af382ca1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "\n",
        "\n",
        "import urllib\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "def linkFetch(unsplash_photo_id):\n",
        "    url = 'https://images.unsplash.com/' + unsplash_photo_id\n",
        "    headers = {'authorization': 'Client-ID tKLJiokp1I7MXnQUwh_UWbkHYIXgxPSTBaT7m2ox1xk'}\n",
        "    \n",
        "    response = requests.get(url, headers=headers)\n",
        "    print(response.text)\n",
        "    #data = response.json()[\"urls\"][\"raw\"]\n",
        "    return data\n",
        "\n",
        "true_image_file = linkFetch('photo-1556796189-d37c540abddb')\n",
        "#true_image = requests.get(true_image_file)\n",
        "#style_image_file = linkFetch('_QxzSVWesm0')\n",
        "#style_image = requests.get(style_image_file)\n",
        "\n",
        "#true_image = Image.open(BytesIO(true_image.content))\n",
        "#style_image = Image.open(BytesIO(style_image.content))\n",
        "#style_image.show()\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "with open('tokens.json', 'r') as f:\n",
        "    data = json.load(f)client_id = data['client_id']\n",
        "client_secret = data['client_secret']redirect_uri = \"\"\n",
        "code = \"\"\n",
        "\n",
        "auth = Auth(client_id, client_secret, redirect_uri, code=code)\n",
        "api = Api(auth)\n",
        "\n",
        "\n",
        "def url_to_image(url):\n",
        "\tphoto = urllib.request.urlopen(url)\n",
        "\timage = Image.open(photo)\n",
        "\treturn image\n",
        "\n",
        "true_image_file = 'https://images.unsplash.com/photo-1556796189-d37c540abddb?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=564&q=80'\n",
        "style_image_file = 'https://unsplash.com/photos/_QxzSVWesm0/download?force=true&w=640'\n",
        "\"\"\"\n",
        "\n",
        "#true_image  = tf.keras.preprocessing.image.img_to_array(true_image_file)\n",
        "#style_image  = tf.keras.preprocessing.image.img_to_array(style_image_file)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-fab444848b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrue_image_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinkFetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'photo-1556796189-d37c540abddb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#true_image = requests.get(true_image_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#style_image_file = linkFetch('_QxzSVWesm0')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-fab444848b99>\u001b[0m in \u001b[0;36mlinkFetch\u001b[0;34m(unsplash_photo_id)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#data = response.json()[\"urls\"][\"raw\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtrue_image_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinkFetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'photo-1556796189-d37c540abddb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqhwN8DJWsPr",
        "outputId": "8ebc6cf2-6650-409b-f1d6-0ff1d02a21a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "style_image_file"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://unsplash.com/photos/_QxzSVWesm0/download?force=true&w=640'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRaOVeedayT4",
        "outputId": "8e1d3617-8376-419c-ecfa-479e4ea68510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "# select images and run program\n",
        "\n",
        "\n",
        "#-- for custom images on Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Public image datasets from tensorflow-datasets\n",
        "#import tensorflow_datasets as tfds\n",
        "#style_image_library = tfds.load('shapes3d', try_gcs=True, download=False)\n",
        "#style_image = style_image_library.take(1)['image']\n",
        "\n",
        "#true_image_library = tfds.load('vgg_face2', try_gcs=True, download=False)\n",
        "#true_image = style_image_library.take(1)['image']\n",
        "# assert isinstance(mnist_train, tf.data.Dataset)\n",
        "\n",
        "\n",
        "# Run program\n",
        "image, itermediary_images = \\\n",
        "    style_transfer(img, img, layers_dict=LAYERS_WEIGHTS)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-e2358a4c732d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Run program\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitermediary_images\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mstyle_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_image_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_image_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLAYERS_WEIGHTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-111256e4a4e5>\u001b[0m in \u001b[0;36mstyle_transfer\u001b[0;34m(image_content, image_style, layers_dict, *kargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# prepare images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimage_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_style\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minit_generated_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_generated_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-edf95e36b9b1>\u001b[0m in \u001b[0;36mprepare_images\u001b[0;34m(image_content, image_style, size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# load images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mstyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_style\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2059\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m             \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1473\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_png\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_png\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid PNG header"
          ]
        }
      ]
    }
  ]
}