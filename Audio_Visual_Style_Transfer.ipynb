{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio Visual Style Transfer",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1C46kA7pkEu71RXyx9mzme-N1F4X0DB1G",
      "authorship_tag": "ABX9TyOKQhaWWpKJoTWR0iURZWUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenouziou/Project_Style_Transfer/blob/master/Audio_Visual_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojqPxubpCb1B"
      },
      "source": [
        "# Audio + Image Style Transfer Model\r\n",
        "\r\n",
        "This app takes a 'content' image and adapts it with texture elements from an audio clip or second image. \r\n",
        "\r\n",
        "Experimental Feature: use an audio clips instead of images. (Functional but still searching for approprate paramaters / audio filter for pleasing results. To try it out, first upload your audio files to '/content/' on Google Colab and update 'AUDIO_OPTIONS' gloabl paramaters below, then run all cells.)\r\n",
        "\r\n",
        "- Try my web app implementation at www.communicatemission.com/ml-projects#style_transfer. (audio not yet implemented)\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "Credits / Attributions:\r\n",
        "\r\n",
        "Using Audio as Style source:\r\n",
        "\r\n",
        "- I have not seen this done before. \r\n",
        "- The basic concept of representing audio as an image in order to apply visual network tools was learned by me from reading *Deep Learning for Coders with fastai & Pytorch* by Howard and Gugger (2020) and also appears in Tensorflow documentation https://www.tensorflow.org/tutorials/audio/simple_audio.\r\n",
        "\r\n",
        "Image Content & Style Models:\r\n",
        "\r\n",
        "- This has been been coded heavily relying on techniques from DeepLearning.ai specialization on Coursera (www.coursera.org/learn/nlp-sequence-models) and the Tensorflow style transfer documentation (www.tensorflow.org/tutorials/generative/style_transfer), which are in turn based on a paper by Gatys, Ecker and Bethge (https://arxiv.org/abs/1508.06576). The web app is built on the Anvil platform.*\r\n",
        "\r\n",
        "- Pretrained image models used are loaded from the tf.keras.applications model. This model currently ustilizes VGG19 (citation: *Very Deep Convolutional Networks for Large-Scale Image Recognition (ICLR 2015))*\r\n",
        "\r\n",
        "Images from photographers posted on Unsplash.com :\r\n",
        "\r\n",
        "- <a href=\"https://unsplash.com/@abm25?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">*Aida Batres*</a>\r\n",
        "- <a href=\"https://unsplash.com/@bmowinkel?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">*Brandon Mowinkel*</a>\r\n",
        "- <a href=\"https://unsplash.com/@autumnstudio?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">*Autumn Studio*</a>\r\n",
        "- <a href=\"https://unsplash.com/@joshhild?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">*Josh Hild*</a>\r\n",
        "- <a href=\"https://unsplash.com/@pawel_czerwinski?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">*Paweł Czerwiński*</a>\r\n",
        "- <a href=\"https://unsplash.com/@lucassankey?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">*Lucas Sankey*</a>\r\n",
        "- <a href=\"https://unsplash.com/@ivanana?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">*Ivana La*</a>\r\n",
        "- <a href=\"https://unsplash.com/@charlesdeluvio?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">*Charles Deluvio*</a>\r\n",
        "- <a href=\"https://unsplash.com/@sharonp?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">*Sharon Pittaway*</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dklI1d0SwqMd"
      },
      "source": [
        "#### REQUIRED IMPORTS ####\n",
        "\n",
        "# ML design\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# data handling\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "!pip install librosa -q  # audio processing\n",
        "import librosa\n",
        "\n",
        "# computations\n",
        "from scipy.special import expit, logit\n",
        "\n",
        "# visualization\n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline\n",
        "import IPython.display as display\n",
        "\n",
        "# audio\n",
        "from IPython.display import Audio\n",
        "\n",
        "# file management\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD3yq5L9CZC1"
      },
      "source": [
        "##### GLOBAL VARIABLES\r\n",
        "File directories and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U7vbvcvfeMc"
      },
      "source": [
        "# GLOBAL VARIABLES - Parameters\r\n",
        "PIXELS = 224\r\n",
        "SIZE = (PIXELS, PIXELS)\r\n",
        "CONTENT_WEIGHT = 10.\r\n",
        "STYLE_WEIGHT = 1.\r\n",
        "LAYERS_WEIGHTS = {#2: 0.2,  # choose style layers & weighting to use\r\n",
        "                  #5: 0.2,\r\n",
        "                  #10: 0.2,\r\n",
        "                  12: 0.2,\r\n",
        "                  15: 0.2,\r\n",
        "                  #20: 0.2,\r\n",
        "                  21: .01,  # final layer (MaxPooling2D)\r\n",
        "                  }"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWg0lHHZMKQ5"
      },
      "source": [
        "# GLOBAL VARIABLES - File Structure\n",
        "\n",
        "# 3rd Party Integrations (enabled / disabled)\n",
        "USE_ANVIL = False\n",
        "USE_GOOGLE_DRIVE = False\n",
        "\n",
        "# File Directory Structure\n",
        "# images datasets (Github)\n",
        "IMAGES_DIR = \\\n",
        "    'https://raw.githubusercontent.com/mvenouziou/Style_Transfer/master/'\n",
        "\n",
        "AUDIO_DIR = '/content/'\n",
        "\n",
        "# saved models (Google Drive)\n",
        "GDRIVE_DIR = '/content/gdrive/'\n",
        "FILEPATH = GDRIVE_DIR + 'MyDrive/Colab_Notebooks/models/style_transfer/'\n",
        "CHECKPOINT_DIR = FILEPATH + 'checkpoints/'\n",
        "CACHE_DIR = FILEPATH + 'cache/'\n",
        "GENERATED_IMAGES_DIR = FILEPATH + 'generated_images/'\n",
        "\n",
        "# mount google drive:\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount(GDRIVE_DIR)\n",
        "\n",
        "# connect to Anvil web app server\n",
        "if USE_ANVIL:\n",
        "    # Anvil Web App server\n",
        "    !pip install anvil-uplink\n",
        "    import anvil.server\n",
        "    anvil.server.connect(\"VCUVGCO27TQO6VER2T6YMM4K-FXYOFTHT5SD2KYNR\")\n",
        "    import anvil.media"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQl2c6poVdTQ"
      },
      "source": [
        "# GLOBAL VARIABLES - Images\n",
        "\n",
        "# set default images\n",
        "DEFAULT_CONTENT_IMAGE_NAME = 'protest'\n",
        "DEFAULT_STYLE_IMAGE_NAME = 'flowers'\n",
        "DEFAULT_TRANSFER_MODEL_NAME = 'vgg'\n",
        "\n",
        "# available images (free license images sourced from unsplash.com)\n",
        "IMAGE_OPTIONS = \\\n",
        "    {'dog':'ivana-la-Dhlwt-VOmeM-unsplash.jpg',\n",
        "     'dog2':'charles-deluvio-Mv9hjnEUHR4-unsplash.jpg',\n",
        "     'protest': 'josh-hild-RD0BXCGemEY-unsplash.jpg',\n",
        "     'statue_of_liberty': 'brandon-mowinkel-UGi_Ng56FRI-unsplash.jpg',\n",
        "     'two_color_straight': 'autumn-studio-PaM7SD5wM6g-unsplash.jpg',\n",
        "     'flowers': 'aida-batres-_QxzSVWesm0-unsplash.jpg',\n",
        "     'flowers2': 'sharon-pittaway-iMdsjoiftZo-unsplash.jpg',\n",
        "     'colors_wild': 'pawel-czerwinski-g2Zf3hJyYAc-unsplash.jpg',\n",
        "     'flag': 'lucas-sankey-gdQ_az6CSPo-unsplash.jpg',\n",
        "     }\n",
        "\n",
        "AUDIO_OPTIONS = {#'song1': 'song1.wav',\n",
        "                 'song1': \"01 - Beck - Devil's Haircut.m4a\",\n",
        "                 'song2': \"05 - Beck - Derelict.m4a\"}\n",
        "\n",
        "# audio transforms\n",
        "# Fourier (Librosa)\n",
        "fourier = {'trans_func': librosa.stft,  # Short-time Fourier Transform (STFT)\n",
        "           'inv_trans_func': librosa.istft,  # Short-time Fourier Transform (STFT)\n",
        "           'hop_length': None,\n",
        "           }\n",
        "\n",
        "# Fourier (Tensorflow)\n",
        "# need to update**\n",
        "#fourier_tf = {'trans_func': tf.signal.stft,  # Short-time Fourier Transform (STFT)\n",
        "#            'inv_trans_func': tf.signal.inverse_stft,  # Short-time Fourier Transform (STFT)\n",
        " #           'hop_length': None,\n",
        "  #          }\n",
        "\n",
        "# constant-Q\n",
        "constant_Q = {'trans_func': librosa.cqt,\n",
        "              'inv_trans_func': librosa.icqt,\n",
        "              'hop_length': 1 * (2**6),\n",
        "              }\n",
        "tempogram = {'trans_func': librosa.feature.tempogram,\n",
        "              'inv_trans_func': None,\n",
        "              'hop_length': 512,\n",
        "              }\n",
        "AUDIO_TRANSFORM_PARAMS = {'Fourier': fourier,\n",
        "                          'constant_Q': constant_Q,\n",
        "                          'tempgram': tempogram}\n",
        "\n",
        "# available transfer learning base models\n",
        "TRANSFER_MODEL_OPTIONS = \\\n",
        "    {'vgg': tf.keras.applications.VGG19, # ImageNet VGG Very Deep 19\n",
        "    #'mobile_net': tf.keras.applications.MobileNetV3Small,\n",
        "     }"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6Lxdj0nE5dn"
      },
      "source": [
        "#### Load pretrained VGG net as base model for transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOufVsiKwzQf"
      },
      "source": [
        "# load pre-trained model for extracting style layers\n",
        "def load_base_model(transfer_model_name, \n",
        "                    transfer_model_options=TRANSFER_MODEL_OPTIONS):\n",
        "    \n",
        "    \"\"\"\n",
        "    ##### parameters set for using VGG\n",
        "    To do: add more model options\n",
        "    \"\"\"\n",
        "    # select model\n",
        "    model = transfer_model_options[transfer_model_name]\n",
        "\n",
        "    # load model\n",
        "    source_model = model(include_top=False, \n",
        "                         weights='imagenet', \n",
        "                         input_tensor=None, \n",
        "                         input_shape=None, \n",
        "                         pooling=None, \n",
        "                         classifier_activation='softmax')\n",
        "   \n",
        "    # lock model for transfer learning\n",
        "    source_model.trainable = False  \n",
        "\n",
        "    return source_model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCiL1Spw5U08"
      },
      "source": [
        "Get source contant / style files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Giczk1cb5UHK"
      },
      "source": [
        "def load_image_from_URL(image_name, \r\n",
        "                        images_directory=IMAGES_DIR,\r\n",
        "                        image_options=IMAGE_OPTIONS,\r\n",
        "                        cache_dir=CACHE_DIR):\r\n",
        "    \r\n",
        "    # choose from saved options\r\n",
        "    filename = image_options[image_name]\r\n",
        "    image_path = tf.keras.utils.get_file(filename, \r\n",
        "                                    origin=images_directory + filename,\r\n",
        "                                    cache_subdir=cache_dir)\r\n",
        "\r\n",
        "    return image_path\r\n",
        "\r\n",
        "\r\n",
        "def load_audio_from_URL(audio_name, \r\n",
        "                       audio_directory=AUDIO_DIR,\r\n",
        "                       audio_options=AUDIO_OPTIONS):\r\n",
        "    \r\n",
        "    # choose from saved options\r\n",
        "    filename = audio_options[audio_name]\r\n",
        "    audio_clip = librosa.load(path = audio_directory + filename, \r\n",
        "                              sr=22050, mono=True, offset=0.0, \r\n",
        "                              duration=20, #\r\n",
        "                              res_type='kaiser_best')\r\n",
        "    audio_clip = audio_clip[0]\r\n",
        "    print('audio_clip:', audio_clip.shape)\r\n",
        "\r\n",
        "\r\n",
        "    return audio_clip"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6U2uEonFDn5"
      },
      "source": [
        "#### Image Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VpksbM6PyfI"
      },
      "source": [
        "# load and prepare images for model\n",
        "def prepare_images(image_path, size):\n",
        "    \"\"\" converts images to tensors and standardizes for model \"\"\"\n",
        "\n",
        "    # load images as PIL and resize\n",
        "    image = tf.keras.preprocessing.image.load_img(image_path, target_size=size)\n",
        "\n",
        "    # convert to array and standardize\n",
        "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "    image = image / 255.\n",
        "\n",
        "    # add batch dimension and convert to tensor: (1, height, width, channels)\n",
        "    image = tf.expand_dims(image, axis=0)\n",
        "\n",
        "    # record original image (relative) dimensions \n",
        "    # (to produce undistorted final image)\n",
        "    orig_image = tf.keras.preprocessing.image.load_img(image_path, target_size=None)\n",
        "    orig_image = tf.keras.preprocessing.image.img_to_array(orig_image)\n",
        "    orig_shape = orig_image.shape[:2]\n",
        "\n",
        "    return image, orig_shape\n",
        "\n",
        "\n",
        "def initialize_generated_image(input_image, noise_rate):\n",
        "    \"\"\" creates noisy version of content image \"\"\"\n",
        "\n",
        "    # define random noise\n",
        "    noise = tf.random.uniform(shape=input_image.shape, minval=-1, maxval=1)\n",
        "    \n",
        "    # update image\n",
        "    image = noise * noise_rate + input_image * (1 - noise_rate)\n",
        "\n",
        "    # convert to tensor\n",
        "    image_tensor = tf.convert_to_tensor(image)\n",
        "    \n",
        "    return image_tensor"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yy2xloq5Ztr"
      },
      "source": [
        "Audio Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCXi1m2z5bw0"
      },
      "source": [
        "def audio_transformed(audio_clip, transform='Fourier', \r\n",
        "                   transform_params=AUDIO_TRANSFORM_PARAMS):\r\n",
        "\r\n",
        "    # get transform paramaters\r\n",
        "    params = transform_params[transform]\r\n",
        "    trans_func = params['trans_func']\r\n",
        "    hop_length = params['hop_length']\r\n",
        "\r\n",
        "    # apply transform to audio clip\r\n",
        "    transformed = trans_func(audio_clip,\r\n",
        "                            hop_length=hop_length )  \r\n",
        "\r\n",
        "    print('transformed:', tf.shape(transformed))\r\n",
        "\r\n",
        "    return transformed\r\n",
        "\r\n",
        "\r\n",
        "def complex_transform_to_image(transformed):\r\n",
        "    \"\"\" convert complex valued matrix (array) into an RGB representation.\r\n",
        "    Can apply audio filters to extract audio style elements \"\"\"\r\n",
        "    \r\n",
        "\r\n",
        "    # apply filters / extract features\r\n",
        "    filter = librosa.filters.chroma(sr=22050, n_fft=2048)\r\n",
        "    transformed = np.dot(filter, transformed)\r\n",
        "\r\n",
        "    # convert to RGB\r\n",
        "    rgb = np.dstack((transformed.real, transformed.imag, \r\n",
        "                     np.abs(transformed)))\r\n",
        "\r\n",
        "    # map values into interval [0, 1]\r\n",
        "    # using scipy.special.expit (sigmoid function)\r\n",
        "    #rgb = expit(rgb)\r\n",
        "    rgb = (rgb + np.min(rgb)) / np.max(rgb + np.min(rgb))\r\n",
        "    \r\n",
        "    # add batch dimension: (1, r, g, b)\r\n",
        "    image_tensor = tf.expand_dims(rgb, axis=0)\r\n",
        "\r\n",
        "    return image_tensor\r\n",
        "\r\n",
        "\r\n",
        "def audio_to_image(audio_clip, size=SIZE, show_image=True, transform='Fourier', \r\n",
        "                   transform_params=AUDIO_TRANSFORM_PARAMS):\r\n",
        "    \"\"\" converts an audio clip to a spectrogram-type image (tensor).\"\"\"\r\n",
        "\r\n",
        "    \r\n",
        "    # apply lossless audio to array transformation\r\n",
        "    transformed = audio_transformed(audio_clip, transform='Fourier', \r\n",
        "                                    transform_params=AUDIO_TRANSFORM_PARAMS)\r\n",
        "    \r\n",
        "    # convert Real and Imaginary components into an RGB representation\r\n",
        "    image_tensor = complex_transform_to_image(transformed)\r\n",
        "\r\n",
        "    # note: resizing for pre-trained model input will occur in loss function\r\n",
        "    # so full res audio can be used for content cost\r\n",
        "\r\n",
        "    return image_tensor[0], transformed\r\n",
        "\r\n",
        "\r\n",
        "def audio_player(audio_clip):\r\n",
        "    # get audio player using IPython.display.Audio\r\n",
        "    file_player = Audio(np.sin(audio_clip), rate=22050, autoplay=True)\r\n",
        "    return file_player\r\n",
        "\r\n",
        "\r\n",
        "def tensor_to_audio(tensor, transform='Fourier', \r\n",
        "                    transform_params=AUDIO_TRANSFORM_PARAMS):\r\n",
        "    \"\"\" applied inverse transform on tensor\r\n",
        "     Returns: audio (as numpy array), audio file player \"\"\"\r\n",
        "\r\n",
        "    # drop batch dim and convert to numpy\r\n",
        "    transformed_array = tf.squeeze(tensor).numpy()\r\n",
        "\r\n",
        "    # reverse STFT representation\r\n",
        "    # ## get paramaters\r\n",
        "    params = transform_params[transform]\r\n",
        "    trans_func = params['inv_trans_func']\r\n",
        "    hop_length = params['hop_length']\r\n",
        "    \r\n",
        "    # ## apply transformation\r\n",
        "    audio_clip = trans_func(transformed_array, hop_length=hop_length)\r\n",
        "\r\n",
        "    # get audio player using IPython.display.Audio\r\n",
        "    file_player = audio_player(audio_clip)\r\n",
        "    \r\n",
        "    return audio_clip, file_player"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMz60aM2Y1lV"
      },
      "source": [
        "Image viewer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF67aMprY0Hf"
      },
      "source": [
        "def view_tensor_as_image(tensor, output_shape, show=True, already_numpy=False): \r\n",
        "    \r\n",
        "    if already_numpy:  # convert numpy arrays to tensor\r\n",
        "        tensor = tf.constant(tensor)\r\n",
        "    \r\n",
        "    # rescale [0, 1] RGB values to [0, 255]\r\n",
        "    tensor_image = tensor * 255\r\n",
        "    \r\n",
        "    # remove dummy dimension (representing batch_size component)\r\n",
        "    tensor_image = tf.squeeze(tensor_image)\r\n",
        "\r\n",
        "    # rescale dimensions\r\n",
        "    output_shape = list(output_shape)\r\n",
        "    tensor_image = tf.image.resize(images=tensor_image, \r\n",
        "                                   size=output_shape)\r\n",
        "    \r\n",
        "    # convert image\r\n",
        "    image_array = tensor_image.numpy()\r\n",
        "    image_array = np.squeeze(image_array)\r\n",
        "    image = tf.keras.preprocessing.image.array_to_img(\r\n",
        "                    image_array, data_format=None, scale=True, dtype=None)\r\n",
        "\r\n",
        "    # show image\r\n",
        "    if show:\r\n",
        "        display.clear_output(wait=True)\r\n",
        "        display.display(image)\r\n",
        "    return image, tensor_image, image_array"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "205pBhpfFHcM"
      },
      "source": [
        "#### Custom Cost Functions\r\n",
        "Measures the differences between generated image and 1) content image and 2) style layers from style image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D-5ZCrQb7Tv"
      },
      "source": [
        "### Cost Functions - Image Content\r\n",
        "# this cost component helps generated image resemble the content image\r\n",
        "def content_cost(learned_tensor, orig_tensor):\r\n",
        "\r\n",
        "    # scale factor to make invariant to input dimensions\r\n",
        "    # value similar to that recommended in DeepLearning.AI course\r\n",
        "    scale_factor = 1 / (tf.math.reduce_prod(tf.shape(orig_tensor)))\r\n",
        "    scale_factor = tf.cast(scale_factor, dtype=tf.float32)\r\n",
        "\r\n",
        "    # compute cost as distance between source image and generated image\r\n",
        "    distance = tf.cast(learned_tensor - orig_tensor, dtype=tf.float32)\r\n",
        "    cost = scale_factor * tf.norm(tensor=distance, ord='euclidean')\r\n",
        "\r\n",
        "    return cost"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gicnBEqLw1IX"
      },
      "source": [
        "### Cost Functions - Gradient Content (represents image style)\n",
        "# this cost component helps generated image resemble the style image's texture\n",
        "# it is applied to interior image layer activation(s)\n",
        "\n",
        "# measures the difference in gradients between image and generated images\n",
        "def gram_matrix(image):\n",
        "    \"\"\"\n",
        "    Computes Gram Matrix\n",
        "    Parameter: tensors of shape (px_width, px_height, channels)\n",
        "    \"\"\"\n",
        "\n",
        "    num_channels = image.shape[-1]\n",
        "    image = tf.reshape(image, [-1, num_channels])\n",
        "    gram = tf.linalg.matmul(image, image, transpose_a=True, transpose_b=False)\n",
        "\n",
        "    return gram\n",
        "\n",
        "\n",
        "# adds up gram_matrix across chosen layers\n",
        "def style_cost(style_activations, generated_activations, weights):\n",
        "\n",
        "    # initialize cost\n",
        "    cost = 0\n",
        "\n",
        "    for layer in weights.keys():\n",
        "\n",
        "        # get content data\n",
        "        style_activ = style_activations[layer]\n",
        "        generated_activ = generated_activations[layer]\n",
        "        weight = weights[layer]\n",
        "\n",
        "        # scale factor to make invariant to input dimensions\n",
        "        # (uses squared terms to balance out Gram matrix, which is essentially square term)\n",
        "        # value asimilar to that recommended in DeepLearning.AI course\n",
        "        scale_factor = 1 / (tf.math.reduce_prod(tf.shape(style_activ))**2)\n",
        "        scale_factor = tf.cast(scale_factor, dtype=tf.float32)\n",
        "\n",
        "       # compute distance between style and generated gradients\n",
        "        gram_style = gram_matrix(style_activ)\n",
        "        gram_generated = gram_matrix(generated_activ)\n",
        "        distance = gram_style - gram_generated\n",
        "\n",
        "       # compute cost\n",
        "        cost += weight * scale_factor * tf.norm(distance, ord=2)**2\n",
        "\n",
        "    return cost"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RwLAsNddJoV"
      },
      "source": [
        "#### Forward Pass Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f64C9GMDi7l5"
      },
      "source": [
        "Style Activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyfYRX7xPjto"
      },
      "source": [
        "def learn_activations(input_tensor, orig_model, layer_nums):\n",
        "    \"\"\"\n",
        "    Conducts forward pass of given model and stores activations\n",
        "    indicated by layers list.\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize activations container\n",
        "    activations_computed = dict()\n",
        "\n",
        "    # forward passes through selected layers\n",
        "    x = input_tensor\n",
        "\n",
        "    for k in range(len(layer_nums)):\n",
        "        if k==0:\n",
        "            start_layer = 0\n",
        "        else:\n",
        "            start_layer = layer_nums[k-1] + 1\n",
        "            \n",
        "        end_layer = layer_nums[k]\n",
        "        \n",
        "        # travel through model to next desired activation layer\n",
        "        for layer_number in range(start_layer, end_layer+1):\n",
        "            x = orig_model.layers[layer_number](x)\n",
        "        \n",
        "        # record activation\n",
        "        activations_computed[end_layer] = x\n",
        "\n",
        "    return activations_computed"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZA79euBi9vh"
      },
      "source": [
        "Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEeX6Fa9i0gO"
      },
      "source": [
        "def loss_function(learned_tensor, orig_content_tensor, \r\n",
        "                  cnn_model, style_activations, layers_dict,\r\n",
        "                  content_weight, style_weight):\r\n",
        "\r\n",
        "    # prepare tensor's image representation\r\n",
        "    if len(learned_tensor.shape) == 4:  # true for images\r\n",
        "        learned_image = learned_tensor\r\n",
        "\r\n",
        "        # clip image values to stay in [0,1] \r\n",
        "        learned_image = tf.clip_by_value(\r\n",
        "                            learned_image, \r\n",
        "                            clip_value_min=0.0, \r\n",
        "                            clip_value_max=1.0\r\n",
        "                            )\r\n",
        "    \r\n",
        "    else:  # for audio\r\n",
        "        # convert to image representation\r\n",
        "        learned_image = complex_transform_to_image(learned_tensor.numpy())\r\n",
        "        \r\n",
        "        # reduce image to size required by pretrained image network\r\n",
        "        learned_image = tf.image.resize(learned_image, size=(224, 224),\r\n",
        "                                method='bilinear')\r\n",
        "    \r\n",
        "    # get parameters\r\n",
        "    layers = list(layers_dict.keys())\r\n",
        "    \r\n",
        "    # get activations\r\n",
        "    generated_activations = \\\r\n",
        "        learn_activations(learned_image, cnn_model, layers)\r\n",
        "        \r\n",
        "    # compute loss\r\n",
        "    loss = content_weight * content_cost(learned_tensor, \r\n",
        "                                         orig_content_tensor) + \\\r\n",
        "            style_weight * style_cost(style_activations, \r\n",
        "                                        generated_activations, \r\n",
        "                                        layers_dict)        \r\n",
        "\r\n",
        "    return loss, learned_image"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nYCqwlsjAs3"
      },
      "source": [
        "Training Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0dItK3Mi__2"
      },
      "source": [
        "def training_step(learned_tensor, orig_content_tensor, \r\n",
        "                  cnn_model, optimizer, \r\n",
        "                  style_activations, layers_dict,\r\n",
        "                  content_weight, style_weight):\r\n",
        "    \r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        tape.watch(learned_tensor)\r\n",
        "\r\n",
        "        # get loss\r\n",
        "        loss, learned_image = \\\r\n",
        "                loss_function(learned_tensor=learned_tensor,\r\n",
        "                              orig_content_tensor=orig_content_tensor, \r\n",
        "                              cnn_model=cnn_model,\r\n",
        "                              style_activations=style_activations, \r\n",
        "                              layers_dict=layers_dict,\r\n",
        "                              content_weight=content_weight, \r\n",
        "                              style_weight=style_weight)\r\n",
        "\r\n",
        "        # get gradients\r\n",
        "        grads = tape.gradient(loss, learned_tensor)\r\n",
        "\r\n",
        "        # backward pass\r\n",
        "        optimizer.apply_gradients([(grads, learned_tensor)])\r\n",
        "\r\n",
        "    return loss, grads, learned_tensor, learned_image\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XYa7ZYFayTF"
      },
      "source": [
        "def train_generated_image(init_generated_tensor, image_activations, \n",
        "                          style_activations, cnn_model, \n",
        "                          content_weight, style_weight, \n",
        "                          num_iterations, learning_rate, \n",
        "                          orig_content_tensor,\n",
        "                          audio_style_mode=False,\n",
        "                          output_shape=(224, 224),\n",
        "                          layers_dict=LAYERS_WEIGHTS, \n",
        "                          checkpoint_dir=CHECKPOINT_DIR,\n",
        "                          generated_images_dir=GENERATED_IMAGES_DIR):\n",
        "\n",
        "    # initialize containers\n",
        "    computed_images_dict = dict()\n",
        "\n",
        "    # initialize trainable variables \n",
        "    learned_tensor = tf.Variable(init_generated_tensor)\n",
        "    \n",
        "    # init generated image\n",
        "    if len(tf.shape(learned_tensor)) == 4:\n",
        "        learned_image = learned_tensor\n",
        "    else:\n",
        "        learned_image = complex_transform_to_image(learned_tensor.numpy())\n",
        "\n",
        "    # set optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, \n",
        "                                         epsilon=1e-1, amsgrad=True,)\n",
        "    \n",
        "    # (note: custom loss function is defined within training loop)\n",
        "\n",
        "    # show initial generated image\n",
        "    print('Original image:\\n')\n",
        "    view_tensor_as_image(learned_image, output_shape, show=True)\n",
        "\n",
        "    # training loop\n",
        "    for iteration in range(num_iterations):\n",
        "        \n",
        "        # enforce image as trainable variable\n",
        "        # (may convert to 'eager tensor' during computations and cause error)\n",
        "        learned_tensor = tf.Variable(learned_tensor)\n",
        "        \n",
        "        # take one training step\n",
        "        loss, grads, learned_tensor, learned_image = \\\n",
        "                training_step(learned_tensor=learned_tensor, \n",
        "                            orig_content_tensor=orig_content_tensor, \n",
        "                            optimizer=optimizer,\n",
        "                            cnn_model=cnn_model,\n",
        "                            style_activations=style_activations, \n",
        "                            layers_dict=layers_dict,\n",
        "                            content_weight=content_weight,\n",
        "                            style_weight=style_weight)\n",
        "\n",
        "        # save progress and give status updates\n",
        "        # report results / save checkpoints\n",
        "        if iteration % 20 == 0:\n",
        "            \n",
        "            # report status\n",
        "            print(\"iteration:\", iteration)\n",
        "            print(\"loss:\", loss.numpy())\n",
        "\n",
        "            # show generated image\n",
        "            view_tensor_as_image(learned_image, output_shape, show=True)\n",
        "\n",
        "            # save generated image in checkpoint            \n",
        "            if USE_GOOGLE_DRIVE:\n",
        "                \n",
        "                save_filename = checkpoint_dir + 'learned_im_tensor_' + \\\n",
        "                                '_iter_' + str(iteration) + '.pickle'\n",
        "\n",
        "                with open(save_filename, 'wb') as handle:\n",
        "                    pickle.dump(image, handle, \n",
        "                                protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "    # training ends                \n",
        "    # show results\n",
        "    print(\"Final loss:\", loss.numpy())\n",
        "\n",
        "    # convert back to viewable image (tensor)\n",
        "    final_image, tensor_image, image_array = \\\n",
        "        view_tensor_as_image(learned_image, output_shape, show=True)\n",
        "\n",
        "    # save result\n",
        "    if USE_GOOGLE_DRIVE:\n",
        "        save_filename = generated_images_dir + 'learned_im.jpg'\n",
        "        tf.keras.preprocessing.image.save_img(save_filename, \n",
        "                                              learned_image)\n",
        "\n",
        "    return final_image, learned_tensor, image_array\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCYa1QiHGGtX"
      },
      "source": [
        "def style_transfer(saved_variables, content_name, style_name, \n",
        "                   transfer_model_name, learning_rate,num_iterations,\n",
        "                   checkpoint_dir=CHECKPOINT_DIR, content_weight=CONTENT_WEIGHT,\n",
        "                   style_weight=STYLE_WEIGHT):\n",
        "\n",
        "    print('Note: Generated image may take time to converge)\\n' +\n",
        "       'Use of GPU or low number of iterations highly recommended')\n",
        "\n",
        "    \"\"\"\n",
        "    note: \n",
        "    saved_variables = {'activations': activations_dict,\n",
        "                       'layers_dict': layers_dict,\n",
        "                       'init_generated_image': init_generated_image,\n",
        "                       'source_tensors': source_tensors_dict,\n",
        "                       'transfer_model': transfer_model,\n",
        "                       }\n",
        "    \"\"\"\n",
        "\n",
        "    # get transfer base model\n",
        "    transfer_model = saved_variables['transfer_model'][transfer_model_name]\n",
        "    \n",
        "    # get prepared images\n",
        "    content_tensor = saved_variables['source_tensors'][content_name]\n",
        "    style_tensor = saved_variables['source_tensors'][style_name]\n",
        "    init_generated_tensor = \\\n",
        "        saved_variables['init_generated_tensor'][content_name]\n",
        "\n",
        "    output_shape = saved_variables['output_shape'][content_name]\n",
        "    \n",
        "    # get CNN layer numbers for transfer\n",
        "    layers_dict = saved_variables['layers_dict']\n",
        "\n",
        "    # get activations based on pretrained transfer model\n",
        "    # (these are held constant throughout training loop)\n",
        "    image_activations = saved_variables['activations'][content_name]\n",
        "    style_activations = saved_variables['activations'][style_name]\n",
        "\n",
        "    # generate new image / train model\n",
        "    learned_image = \\\n",
        "        train_generated_image(init_generated_tensor=init_generated_tensor, \n",
        "                              image_activations=image_activations, \n",
        "                              style_activations=style_activations, \n",
        "                              cnn_model=transfer_model, \n",
        "                              layers_dict=layers_dict, \n",
        "                              content_weight=content_weight, \n",
        "                              style_weight=style_weight, \n",
        "                              num_iterations=num_iterations,\n",
        "                              learning_rate=learning_rate,\n",
        "                              orig_content_tensor=content_tensor,\n",
        "                              output_shape=output_shape,\n",
        "                              checkpoint_dir=checkpoint_dir\n",
        "                              )\n",
        "\n",
        "    return learned_image"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu8u8dBP_Lnh"
      },
      "source": [
        "Preload prepared image / models\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMcwa3p2_FNz"
      },
      "source": [
        "# Preload prepared image / models\r\n",
        "\r\n",
        "def preload_variables(transfer_model_name=DEFAULT_TRANSFER_MODEL_NAME,\r\n",
        "                      transfer_model_options=TRANSFER_MODEL_OPTIONS,\r\n",
        "                      image_options=IMAGE_OPTIONS,\r\n",
        "                      audio_options=AUDIO_OPTIONS,\r\n",
        "                      layers_dict=LAYERS_WEIGHTS,\r\n",
        "                      noise_rate=.1,\r\n",
        "                      size=SIZE):\r\n",
        "\r\n",
        "    # initialize containers\r\n",
        "    transfer_models_dict = {}\r\n",
        "    source_tensors_dict = {}\r\n",
        "    init_generated_tensor_dict = {}\r\n",
        "    activations_dict = {}\r\n",
        "    output_shapes_dict = {}\r\n",
        "\r\n",
        "    # prepare transfer model\r\n",
        "    for model_name in transfer_model_options:\r\n",
        "        transfer_models_dict[model_name] \\\r\n",
        "            = load_base_model(model_name, transfer_model_options)\r\n",
        "\r\n",
        "    # get layer numbers for activation computations\r\n",
        "    layers = list(layers_dict.keys())  \r\n",
        "\r\n",
        "    # process each image\r\n",
        "    for name in {**image_options , **audio_options}:  # merged dicts\r\n",
        "\r\n",
        "        # load images\r\n",
        "        if name in image_options:\r\n",
        "            print(f'loading image: {name}')\r\n",
        "\r\n",
        "            # load and preprocess as tensor\r\n",
        "            image_path = load_image_from_URL(name)\r\n",
        "            source_tensor, output_shape = prepare_images(image_path=image_path, \r\n",
        "                                                        size=size)\r\n",
        "            \r\n",
        "            # initialize corresponding generated image\r\n",
        "            init_generated_tensor = \\\r\n",
        "                initialize_generated_image(source_tensor, noise_rate=noise_rate)\r\n",
        "\r\n",
        "            # record shape (scaled down)\r\n",
        "            max_dim = max(output_shape)\r\n",
        "            scaled_output_shape = (max(size) * output_shape[0]//max_dim, \r\n",
        "                                    max(size) * output_shape[1]//max_dim)\r\n",
        "            output_shapes_dict[name] = scaled_output_shape\r\n",
        "\r\n",
        "            # compute activations\r\n",
        "            model = transfer_models_dict[model_name]\r\n",
        "            activations = learn_activations(source_tensor, model, layers)\r\n",
        "\r\n",
        "        # load audio\r\n",
        "        if name in audio_options:\r\n",
        "            print(f'loading audio: {name}')\r\n",
        "\r\n",
        "            # load and preprocess as tensor\r\n",
        "            audio_clip = load_audio_from_URL(name)\r\n",
        "            rgb_conversion, source_tensor = \\\r\n",
        "                audio_to_image(audio_clip, show_image=False)\r\n",
        "\r\n",
        "            rgb_conversion = tf.expand_dims(rgb_conversion, axis=0)\r\n",
        "\r\n",
        "            # initialize corresponding generated image=           \r\n",
        "            init_generated_tensor = \\\r\n",
        "                initialize_generated_image(source_tensor, noise_rate=0.01)\r\n",
        "        \r\n",
        "            # record shape (not scaled to minimize distortion)\r\n",
        "            output_shapes_dict[name] = (output_shape[0], output_shape[1])\r\n",
        "\r\n",
        "            # compute activations (using audio's image representation)\r\n",
        "            model = transfer_models_dict[model_name]\r\n",
        "            activations = learn_activations(rgb_conversion, model, layers)\r\n",
        "        \r\n",
        "        # save in dict\r\n",
        "        source_tensors_dict[name] = source_tensor\r\n",
        "        init_generated_tensor_dict[name] = init_generated_tensor\r\n",
        "        activations_dict[name] = activations\r\n",
        "\r\n",
        "    # organize outputs\r\n",
        "    out = {'activations': activations_dict,\r\n",
        "           'layers_dict': layers_dict,\r\n",
        "           'init_generated_tensor': init_generated_tensor_dict,\r\n",
        "           'source_tensors': source_tensors_dict,\r\n",
        "           'transfer_model': transfer_models_dict,\r\n",
        "           'output_shape': output_shapes_dict,\r\n",
        "           }\r\n",
        "\r\n",
        "    return out"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTtbcpZxJVAm"
      },
      "source": [
        "Pre-load Images, Model and Activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9bV0VG-IzAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e1b9ced-fc18-4a86-c28b-fe5e3d581318"
      },
      "source": [
        "SAVED_VARIABLES_DICT = preload_variables(\r\n",
        "                            transfer_model_name=DEFAULT_TRANSFER_MODEL_NAME,\r\n",
        "                            transfer_model_options=TRANSFER_MODEL_OPTIONS,\r\n",
        "                            image_options=IMAGE_OPTIONS,\r\n",
        "                            audio_options=AUDIO_OPTIONS,\r\n",
        "                            noise_rate=.1,\r\n",
        "                            size=SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading image: dog\n",
            "loading image: dog2\n",
            "loading image: protest\n",
            "loading image: statue_of_liberty\n",
            "loading image: two_color_straight\n",
            "loading image: flowers\n",
            "loading image: flowers2\n",
            "loading image: colors_wild\n",
            "loading image: flag\n",
            "loading audio: song1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK4W599DIkl7"
      },
      "source": [
        "Final Function for Generating Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6yM-Y8E1Xzh"
      },
      "source": [
        "#@anvil.server.callable\r\n",
        "def generate_image(orig_image=DEFAULT_CONTENT_IMAGE_NAME, \r\n",
        "                   style_image=DEFAULT_STYLE_IMAGE_NAME,\r\n",
        "                   transfer_model_name=DEFAULT_TRANSFER_MODEL_NAME,\r\n",
        "                   saved_variables=SAVED_VARIABLES_DICT,\r\n",
        "                   output_shape=(224,224),\r\n",
        "                   learning_rate=.20,\r\n",
        "                   num_iterations=3):   \r\n",
        "\r\n",
        "    output_shape = saved_variables['output_shape'][orig_image],\r\n",
        "    output_shape = tf.reshape(tf.constant(output_shape), [-1,])\r\n",
        "\r\n",
        "    final_image, learned_tensor, image_array = \\\r\n",
        "        style_transfer(saved_variables=saved_variables,\r\n",
        "                       content_name=orig_image, \r\n",
        "                       style_name=style_image, \r\n",
        "                       transfer_model_name=transfer_model_name,\r\n",
        "                       learning_rate=learning_rate,\r\n",
        "                       num_iterations=num_iterations)\r\n",
        "\r\n",
        "    save_filename = 'temp.jpg'    \r\n",
        "    tf.keras.preprocessing.image.save_img(save_filename, image_array)\r\n",
        "\r\n",
        "    if USE_ANVIL:\r\n",
        "        im_file = anvil.media.from_file(\"temp.jpg\")\r\n",
        "        \r\n",
        "    width, height = tf.shape(learned_tensor)[-2], tf.shape(learned_tensor)[-1]\r\n",
        "    \r\n",
        "    return final_image, width, height, learned_tensor\r\n",
        "\r\n",
        "if USE_ANVIL:\r\n",
        "    anvil.server.wait_forever()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTQYkLMK6-RU"
      },
      "source": [
        "## Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYFcI51a7Ar_"
      },
      "source": [
        "Visual Style Transfer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PBdPAdjsT0h"
      },
      "source": [
        "# Run program\r\n",
        "orig_image = 'flag'\r\n",
        "style_image = 'flowers'\r\n",
        "\r\n",
        "temp_image, _, _, temp_learned_tensor = generate_image(orig_image, \r\n",
        "                            style_image,\r\n",
        "                            learning_rate=.01,\r\n",
        "                            num_iterations=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoMdCS2u7D1_"
      },
      "source": [
        "Audio Style Transfer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxdwfOShem5q"
      },
      "source": [
        "play_original = False\r\n",
        "\r\n",
        "if play_original:\r\n",
        "    # Run program\r\n",
        "    # EXPERIMENTAL / IN PROGRESS: Using audio as style source\r\n",
        "    orig_image = 'song2'\r\n",
        "    style_image = 'song1'\r\n",
        "\r\n",
        "    # initial song\r\n",
        "    clip = load_audio_from_URL(audio_name=orig_image, \r\n",
        "                        audio_directory=AUDIO_DIR,\r\n",
        "                        audio_options=AUDIO_OPTIONS)\r\n",
        "    audio_player(clip)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9xyok8ZuEQl"
      },
      "source": [
        "orig_image = 'song2'\r\n",
        "style_image = 'song1'\r\n",
        "\r\n",
        "temp_image, _, _, temp_learned_tensor = generate_image(orig_image, \r\n",
        "                            style_image,\r\n",
        "                            learning_rate=1e10,\r\n",
        "                            num_iterations=100)\r\n",
        "\r\n",
        "\r\n",
        "# generated song\r\n",
        "audio_clip, file_player = tensor_to_audio(temp_learned_tensor, \r\n",
        "                                          transform='Fourier', \r\n",
        "                                          transform_params=AUDIO_TRANSFORM_PARAMS)\r\n",
        "\r\n",
        "file_player"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9yuJqrV-p-t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}